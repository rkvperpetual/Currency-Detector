{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.17.0\n",
      "Python Version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"Python Version:\", sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow-addons (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow-addons\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been balanced and copied to: C:\\Users\\Raj Kumar\\Downloads\\Balanced_Datasets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Paths to your original and new balanced datasets\n",
    "original_dataset_dir = r'C:\\Users\\Raj Kumar\\Downloads\\archive (6)\\Datasets'\n",
    "balanced_dataset_dir = r'C:\\Users\\Raj Kumar\\Downloads\\Balanced_Datasets'\n",
    "\n",
    "# Create the balanced dataset directory\n",
    "if not os.path.exists(balanced_dataset_dir):\n",
    "    os.makedirs(balanced_dataset_dir)\n",
    "\n",
    "# Loop over all classes (subdirectories in original dataset)\n",
    "class_names = os.listdir(original_dataset_dir)\n",
    "max_class_size = 0\n",
    "\n",
    "# Find the class with the maximum number of images\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(original_dataset_dir, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        num_images = len(os.listdir(class_dir))\n",
    "        if num_images > max_class_size:\n",
    "            max_class_size = num_images\n",
    "\n",
    "# Now, create the balanced dataset by copying images\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(original_dataset_dir, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        # Create a directory for the current class in the balanced dataset\n",
    "        balanced_class_dir = os.path.join(balanced_dataset_dir, class_name)\n",
    "        if not os.path.exists(balanced_class_dir):\n",
    "            os.makedirs(balanced_class_dir)\n",
    "        \n",
    "        # Get all image paths in the current class\n",
    "        image_paths = [os.path.join(class_dir, fname) for fname in os.listdir(class_dir) if fname.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        # Calculate how many images to copy\n",
    "        num_images_to_copy = max_class_size - len(image_paths)\n",
    "        \n",
    "        # If the class has fewer images, duplicate random images to balance\n",
    "        if num_images_to_copy > 0:\n",
    "            # Randomly sample images to duplicate\n",
    "            additional_images = random.choices(image_paths, k=num_images_to_copy)\n",
    "            image_paths += additional_images\n",
    "        \n",
    "        # Copy the images to the balanced dataset directory\n",
    "        for image_path in image_paths:\n",
    "            shutil.copy(image_path, balanced_class_dir)\n",
    "\n",
    "print(\"Dataset has been balanced and copied to:\", balanced_dataset_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_dir = Path(r\"C:\\Users\\Raj Kumar\\Downloads\\archive (6)\\Datasets\")  # Original dataset path\n",
    "balanced_dataset_dir = Path(r\"C:\\Users\\Raj Kumar\\Downloads\\Balanced_Dataset\")  # New balanced dataset path\n",
    "\n",
    "# Create new directory if it doesn't exist\n",
    "balanced_dataset_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: {'fifty_new': 265, 'fifty_old': 234, 'five_hundred': 279, 'hundred_new': 224, 'hundred_old': 107, 'ten_new': 345, 'ten_old': 306, 'twenty_new': 288, 'twenty_old': 257, 'two_hundred': 272, 'two_thousand': 102}\n",
      "Max images in any class: 345\n"
     ]
    }
   ],
   "source": [
    "class_counts = {}\n",
    "\n",
    "# Loop through each class folder\n",
    "for class_name in os.listdir(original_dataset_dir):\n",
    "    class_path = original_dataset_dir / class_name\n",
    "    if os.path.isdir(class_path):\n",
    "        class_counts[class_name] = len(os.listdir(class_path))\n",
    "\n",
    "# Find the maximum number of images in any class\n",
    "max_images = max(class_counts.values())\n",
    "\n",
    "print(\"Original class distribution:\", class_counts)\n",
    "print(f\"Max images in any class: {max_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset created successfully!\n"
     ]
    }
   ],
   "source": [
    "for class_name, count in class_counts.items():\n",
    "    src_class_path = original_dataset_dir / class_name\n",
    "    dest_class_path = balanced_dataset_dir / class_name\n",
    "    dest_class_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Copy existing images\n",
    "    for img_name in os.listdir(src_class_path):\n",
    "        shutil.copy(src_class_path / img_name, dest_class_path / img_name)\n",
    "\n",
    "    # If class is underrepresented, duplicate random images\n",
    "    if count < max_images:\n",
    "        extra_images_needed = max_images - count\n",
    "        images = os.listdir(dest_class_path)\n",
    "        \n",
    "        for i in range(extra_images_needed):\n",
    "            img_to_copy = random.choice(images)  # Pick a random image\n",
    "            new_img_name = f\"copy_{i}_{img_to_copy}\"  # Rename copied image\n",
    "            shutil.copy(dest_class_path / img_to_copy, dest_class_path / new_img_name)\n",
    "\n",
    "print(\"Balanced dataset created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced class distribution: {'fifty_new': 345, 'fifty_old': 345, 'five_hundred': 345, 'hundred_new': 345, 'hundred_old': 345, 'ten_new': 345, 'ten_old': 345, 'twenty_new': 345, 'twenty_old': 345, 'two_hundred': 345, 'two_thousand': 345}\n"
     ]
    }
   ],
   "source": [
    "balanced_class_counts = {class_name: len(os.listdir(balanced_dataset_dir / class_name)) \n",
    "                         for class_name in os.listdir(balanced_dataset_dir)}\n",
    "\n",
    "print(\"Balanced class distribution:\", balanced_class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the new balanced dataset\n",
    "DATASET_PATH = Path(r\"C:\\Users\\Raj Kumar\\Downloads\\Balanced_Dataset\")\n",
    "\n",
    "# Image parameters\n",
    "IMAGE_SIZE = (128, 128)  # Adjust size based on model needs\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3036 images belonging to 11 classes.\n",
      "Found 759 images belonging to 11 classes.\n",
      "Detected 11 classes: {'fifty_new': 0, 'fifty_old': 1, 'five_hundred': 2, 'hundred_new': 3, 'hundred_old': 4, 'ten_new': 5, 'ten_old': 6, 'twenty_new': 7, 'twenty_old': 8, 'two_hundred': 9, 'two_thousand': 10}\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # 20% for validation\n",
    ")\n",
    "\n",
    "# Load training data\n",
    "train_data = datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Load validation data\n",
    "val_data = datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Get number of classes\n",
    "num_classes = len(train_data.class_indices)\n",
    "print(f\"Detected {num_classes} classes: {train_data.class_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raj Kumar\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,211,392</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,419</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_16 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_17 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_18 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_7 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m3,211,392\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)             │         \u001b[38;5;34m1,419\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,306,059</span> (12.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,306,059\u001b[0m (12.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,306,059</span> (12.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,306,059\u001b[0m (12.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    # Convolutional layers\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Flatten and Dense layers\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Prevents overfitting\n",
    "    Dense(num_classes, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 2s/step - accuracy: 0.0966 - loss: 2.3987 - val_accuracy: 0.0870 - val_loss: 2.4212\n",
      "Epoch 2/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 2s/step - accuracy: 0.1589 - loss: 2.3197 - val_accuracy: 0.1528 - val_loss: 2.3073\n",
      "Epoch 3/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 2s/step - accuracy: 0.2367 - loss: 2.1438 - val_accuracy: 0.1897 - val_loss: 2.1792\n",
      "Epoch 4/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 2s/step - accuracy: 0.2837 - loss: 2.0319 - val_accuracy: 0.1910 - val_loss: 2.1379\n",
      "Epoch 5/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 2s/step - accuracy: 0.3371 - loss: 1.9183 - val_accuracy: 0.2596 - val_loss: 2.0358\n",
      "Epoch 6/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 2s/step - accuracy: 0.3433 - loss: 1.8502 - val_accuracy: 0.2754 - val_loss: 2.0147\n",
      "Epoch 7/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.3772 - loss: 1.7923 - val_accuracy: 0.3057 - val_loss: 1.9498\n",
      "Epoch 8/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 2s/step - accuracy: 0.3792 - loss: 1.7026 - val_accuracy: 0.3373 - val_loss: 1.8819\n",
      "Epoch 9/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 2s/step - accuracy: 0.4251 - loss: 1.6132 - val_accuracy: 0.3267 - val_loss: 1.8872\n",
      "Epoch 10/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - accuracy: 0.4364 - loss: 1.5819 - val_accuracy: 0.3386 - val_loss: 1.8795\n",
      "Epoch 11/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 908ms/step - accuracy: 0.4484 - loss: 1.5267 - val_accuracy: 0.3597 - val_loss: 1.7873\n",
      "Epoch 12/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 753ms/step - accuracy: 0.4559 - loss: 1.5050 - val_accuracy: 0.3742 - val_loss: 1.7704\n",
      "Epoch 13/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 746ms/step - accuracy: 0.4839 - loss: 1.4768 - val_accuracy: 0.3939 - val_loss: 1.7332\n",
      "Epoch 14/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 762ms/step - accuracy: 0.4985 - loss: 1.4413 - val_accuracy: 0.3768 - val_loss: 1.7287\n",
      "Epoch 15/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - accuracy: 0.4947 - loss: 1.4173 - val_accuracy: 0.3926 - val_loss: 1.7180\n",
      "Epoch 16/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 2s/step - accuracy: 0.5167 - loss: 1.3805 - val_accuracy: 0.3860 - val_loss: 1.7383\n",
      "Epoch 17/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - accuracy: 0.5424 - loss: 1.3221 - val_accuracy: 0.4177 - val_loss: 1.6641\n",
      "Epoch 18/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 6s/step - accuracy: 0.5345 - loss: 1.3058 - val_accuracy: 0.3966 - val_loss: 1.6777\n",
      "Epoch 19/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 750ms/step - accuracy: 0.5534 - loss: 1.2821 - val_accuracy: 0.4097 - val_loss: 1.6478\n",
      "Epoch 20/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 699ms/step - accuracy: 0.5546 - loss: 1.2633 - val_accuracy: 0.4177 - val_loss: 1.6465\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "model.save(\"currency_detector4.h5\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 454ms/step - accuracy: 0.4403 - loss: 1.5913\n",
      "Validation Accuracy: 41.37%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(val_data)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fifty_new': 0, 'fifty_old': 1, 'five_hundred': 2, 'hundred_new': 3, 'hundred_old': 4, 'ten_new': 5, 'ten_old': 6, 'twenty_new': 7, 'twenty_old': 8, 'two_hundred': 9, 'two_thousand': 10}\n"
     ]
    }
   ],
   "source": [
    "print(train_data.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_labels = [\n",
    "    \"₹50\", \"₹50\", \"₹500\", \"₹100\", \"₹100\", \n",
    "    \"₹10\", \"₹10\", \"20\", \"20\", \"200\", \"200\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the image\n",
    "def preprocess_image(img_path, target_size=(128, 128)):\n",
    "    img = image.load_img(img_path, target_size=target_size)  # Load image\n",
    "    img_array = image.img_to_array(img)  # Convert to array\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Expand dims for batch\n",
    "    img_array = img_array / 255.0  # Normalize\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_currency(img_path):\n",
    "    img_array = preprocess_image(img_path)\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class = np.argmax(prediction)  # Get class with highest probability\n",
    "    confidence = np.max(prediction)  # Get confidence score\n",
    "    \n",
    "    currency_name = currency_labels[predicted_class]  # Map index to currency name\n",
    "    print(f\"Predicted Currency: {currency_name} (Confidence: {confidence * 100:.2f}%)\")\n",
    "    return currency_name, confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "Predicted Currency: ₹100 (Confidence: 37.26%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('₹100', 0.3726126)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = r\"C:\\Users\\Raj Kumar\\Downloads\\notes\\2001.jpg\"  # Change as needed\n",
    "predict_currency(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"currency_detector4.h5\")\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class labels (same order as in train_data.class_indices)\n",
    "class_labels = ['class_0', 'class_1', 'class_2', 'class_3', 'class_4', \n",
    "                'class_5', 'class_6', 'class_7', 'class_8', 'class_9', 'class_10']\n",
    "\n",
    "# You should replace the above list with your actual class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path, target_size=(128, 128)):\n",
    "    img = image.load_img(img_path, target_size=target_size)  # Load image\n",
    "    img_array = image.img_to_array(img)  # Convert to array\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Expand dims for batch\n",
    "    img_array = img_array / 255.0  # Normalize\n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_currency(img_path):\n",
    "    img_array = preprocess_image(img_path)\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class = np.argmax(prediction)  # Get class with highest probability\n",
    "    confidence = np.max(prediction)  # Get confidence score\n",
    "    \n",
    "    print(f\"Predicted Class: {class_labels[predicted_class]} (Confidence: {confidence * 100:.2f}%)\")\n",
    "    return class_labels[predicted_class], confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "Predicted Class: class_1 (Confidence: 53.28%)\n"
     ]
    }
   ],
   "source": [
    "img_path = r\"C:\\Users\\Raj Kumar\\Downloads\\5001.jpg\"  # Replace with actual path\n",
    "predicted_class, confidence = predict_currency(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
